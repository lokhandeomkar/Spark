I feel that both platforms have their strengths and weaknesses.In MR, we could choose not to output a key value pair from a map task for every input that it received. In Spark, I did not see that functionality. Also, in MR, writing multiple KV pairs for every map input was intuitive whereas it was not so in Spark. I needed to use a PairFlatMapFunction. Having said that, there was just a lot more functionality in Spark that one could exploit. Some of the APIs like groupByKey(), sortByKey(), values(), distinct() came in really handy. I also felt that due to there being a lot of things one could use, there was not enough documentation. Also, some of the functionality has been discontinued and online examples were based on that, so it was a bit annoying to see that you cannot use the functionality given in the online example.Chaining MR jobs was a bit of a pain whereas in Spark you can just imagine there being one pipeline of operations the data goes through and that is more intuitive. For example in task 2, reducers were eliminated by using APIs like groupByKey(), sortByKey() etc.I believe that any ease I had using MR over using Spark can be attributed to a higher learning curve for Spark given its extended functionality over MR, but once I got over that, I enjoyed using Spark, for later tasks. 